 ğŸŒ URL Crawler Dashboard

A full-stack web application to crawl and analyze website URLs. It extracts structured data like heading tags, link stats, broken links, and login form presence with a polished, user-friendly dashboard.

---

## âš™ï¸ Features

- âœ… Add and queue website URLs for crawling
- ğŸ” Extract and display:
  - HTML version
  - Page title
  - Counts of headings (H1â€“H6)
  - Internal & external link counts
  - Broken link count
  - Login form detection
- â™»ï¸ Re-analyze any URL
- ğŸ—‘ï¸ Delete any queued or completed URL
- ğŸ“Š Pie chart visualization of link types
- ğŸ’¡ Responsive UI with Tailwind CSS

---

## ğŸ§± Tech Stack

- **Frontend:** React, TypeScript, React Query, React Router, Tailwind CSS, Recharts
- **Backend:** Go (Gin), GORM, MySQL
- **API:** REST
- **Other:** Axios, dotenv, HTML parsing

---

## ğŸ“ Folder Structure

.
â”œâ”€â”€ crawler-api/ # Go backend
â”‚ â”œâ”€â”€ internal/
â”‚ â”‚ â”œâ”€â”€ crawler/ # Core crawling logic
â”‚ â”‚ â”œâ”€â”€ db/ # DB connection setup
â”‚ â”‚ â””â”€â”€ models/ # GORM models
â”‚ â””â”€â”€ main.go
â”‚
â”œâ”€â”€ crawler-frontend/ # React frontend
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ components/ # Layout, AddUrlForm, UrlTable
â”‚ â”‚ â”œâ”€â”€ pages/ # Dashboard, UrlDetails
â”‚ â”‚ â””â”€â”€ types/ # TypeScript types
â”‚ â””â”€â”€ App.tsx


---

## ğŸš€ Getting Started

### ğŸ› ï¸ Backend Setup

1. Start MySQL and create a database:

```sql
CREATE DATABASE crawler_db;
Inside crawler-api/, create a .env file:
DB_USER=root
DB_PASSWORD=yourpassword
DB_NAME=crawler_db
DB_HOST=localhost
PORT=8080
Run the Go server:
cd crawler-api
go run main.go
ğŸ’» Frontend Setup
Install and run:
cd crawler-frontend
npm install
npm run dev
Open the app in your browser at:
http://localhost:3000
ğŸ§ª How to Use

Enter a URL (e.g., https://example.com) in the form.
Wait a few seconds for analysis to complete.
View results on the dashboard.
Click "View" for details with pie chart.
Use "Re-analyze" or "Delete" for any listed URL.
